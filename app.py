import streamlit as st
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
from model import Dual
from common import OCBAM

# ==========================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# ==========================================
# L∆ØU √ù: B·∫°n c·∫ßn s·ª≠a l·∫°i danh s√°ch n√†y ƒë√∫ng theo th·ª© t·ª± alphabet
# c·ªßa c√°c th∆∞ m·ª•c trong dataset VNEMOS l√∫c b·∫°n train.
# V√≠ d·ª•: n·∫øu folder l√† 'angry', 'fear', 'happiness', 'neutral', 'sadness'
# th√¨ th·ª© t·ª± s·∫Ω l√†:
EMO_CLASSES = {
    0: "Angry (T·ª©c gi·∫≠n)",
    1: "Fear (S·ª£ h√£i)",
    2: "Happiness (Vui v·∫ª)",
    3: "Neutral (B√¨nh th∆∞·ªùng)",
    4: "Sadness (Bu·ªìn b√£)"
}

DEVICE = torch.device("cpu") # Ch·∫°y demo tr√™n CPU cho ·ªïn ƒë·ªãnh
MAX_LEN = 100 # ƒê·ªô d√†i chu·ªói MFCC (ph·∫£i kh·ªõp v·ªõi l√∫c train)

# ==========================================
# 2. C√ÅC H√ÄM X·ª¨ L√ù
# ==========================================
@st.cache_resource
def load_model():
    """Load model m·ªôt l·∫ßn duy nh·∫•t ƒë·ªÉ d√πng m√£i m√£i"""
    # Kh·ªüi t·∫°o model v·ªõi s·ªë l·ªõp l√† 5 (nh∆∞ k·∫øt qu·∫£ debug c·ªßa b·∫°n)
    model = Dual(num_classes=len(EMO_CLASSES))
    try:
        model.load_state_dict(torch.load("best_model.pth", map_location=DEVICE))
        model.to(DEVICE)
        model.eval()
        return model
    except Exception as e:
        st.error(f"L·ªói load model: {e}")
        return None

def preprocess_audio(file_path):
    """Bi·∫øn file √¢m thanh th√†nh Tensor ƒë·∫ßu v√†o cho m√¥ h√¨nh"""
    try:
        # Load file (ch·ªâ l·∫•y 3 gi√¢y ƒë·∫ßu ƒë·ªÉ x·ª≠ l√Ω nhanh)
        y, sr = librosa.load(file_path, sr=16000, duration=3.0)
        
        # Tr√≠ch xu·∫•t MFCC
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
        
        # Padding ho·∫∑c c·∫Øt ng·∫Øn cho ƒë√∫ng chu·∫©n MAX_LEN
        if mfcc.shape[1] < MAX_LEN:
            pad_width = MAX_LEN - mfcc.shape[1]
            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')
        else:
            mfcc = mfcc[:, :MAX_LEN]
            
        # Chuy·ªÉn th√†nh Tensor 4 chi·ªÅu: (Batch, Channel, Height, Width)
        # (1, 1, 40, 100)
        mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        return mfcc_tensor
    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω √¢m thanh: {e}")
        return None

# ==========================================
# 3. GIAO DI·ªÜN CH√çNH (MAIN APP)
# ==========================================
def main():
    st.set_page_config(page_title="Emotion AI Demo", page_icon="üéôÔ∏è")
    
    st.title("üéôÔ∏è Nh·∫≠n Di·ªán C·∫£m X√∫c Gi·ªçng N√≥i (VNEMOS)")
    st.write("H·ªá th·ªëng s·ª≠ d·ª•ng m√¥ h√¨nh Deep Learning (CNN + Attention) ƒë·ªÉ ph√¢n t√≠ch gi·ªçng n√≥i ti·∫øng Vi·ªát.")
    
    # Load model
    with st.spinner("ƒêang kh·ªüi ƒë·ªông AI..."):
        model = load_model()
        
    if model is None:
        st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ ch·∫°y ·ª©ng d·ª•ng do l·ªói Model.")
        st.stop()

    st.divider()

    # --- C·ªôt tr√°i: Input ---
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("1. ƒê·∫ßu v√†o")
        uploaded_file = st.file_uploader("T·∫£i l√™n file ghi √¢m (.wav)", type=["wav", "mp3"])
        
        # (T√πy ch·ªçn) Ghi √¢m tr·ª±c ti·∫øp - Ch·ªâ ho·∫°t ƒë·ªông tr√™n Streamlit m·ªõi nh·∫•t
        audio_input = st.audio_input("Ho·∫∑c ghi √¢m tr·ª±c ti·∫øp") if hasattr(st, "audio_input") else None

    # X√°c ƒë·ªãnh file ƒë·ªÉ x·ª≠ l√Ω
    file_to_process = uploaded_file if uploaded_file else audio_input

    # --- C·ªôt ph·∫£i: K·∫øt qu·∫£ ---
    with col2:
        st.subheader("2. Ph√¢n t√≠ch")
        
        if file_to_process:
            # Nghe l·∫°i file
            st.audio(file_to_process, format="audio/wav")
            
            if st.button("üöÄ Ch·∫°y M√¥ h√¨nh", type="primary"):
                with st.spinner("ƒêang ph√¢n t√≠ch t√≠n hi·ªáu..."):
                    # 1. Ti·ªÅn x·ª≠ l√Ω
                    input_tensor = preprocess_audio(file_to_process)
                    
                    if input_tensor is not None:
                        input_tensor = input_tensor.to(DEVICE)
                        
                        # 2. D·ª± ƒëo√°n
                        with torch.no_grad():
                            output = model(input_tensor)
                            # T√≠nh x√°c su·∫•t (Softmax)
                            probs = torch.nn.functional.softmax(output, dim=1)
                            confidence, predicted = torch.max(probs, 1)
                        
                        # 3. Hi·ªÉn th·ªã k·∫øt qu·∫£
                        idx = predicted.item()
                        label = EMO_CLASSES.get(idx, "Kh√¥ng x√°c ƒë·ªãnh")
                        score = confidence.item() * 100
                        
                        # H·ªôp k·∫øt qu·∫£ n·ªïi b·∫≠t
                        st.success(f"### K·∫øt qu·∫£: {label}")
                        st.info(f"ƒê·ªô tin c·∫≠y: **{score:.2f}%**")
                        
                        # 4. V·∫Ω bi·ªÉu ƒë·ªì c·ªôt x√°c su·∫•t
                        st.write("Chi ti·∫øt x√°c su·∫•t c√°c l·ªõp:")
                        chart_data = {
                            name: prob.item() 
                            for i, (name, prob) in enumerate(zip(EMO_CLASSES.values(), probs[0]))
                        }
                        st.bar_chart(chart_data)
        else:
            st.info("üëà Vui l√≤ng t·∫£i file ho·∫∑c ghi √¢m ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

if __name__ == "__main__":
    main()